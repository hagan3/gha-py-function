{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pandas  0.25.1\n",
      "Numpy  1.16.2\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config IPCompleter.greedy=True\n",
    "import warnings\n",
    "# Squash warning messages\n",
    "warnings.showwarning = lambda *args, **kwargs: None\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import sklearn\n",
    "from sklearn import cluster\n",
    "from sklearn import ensemble\n",
    "from sklearn import feature_selection\n",
    "from sklearn import metrics\n",
    "from sklearn import model_selection\n",
    "from sklearn import neural_network\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import davies_bouldin_score\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import FastICA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from ipywidgets import widgets\n",
    "from ipywidgets import HBox, Label\n",
    "from IPython.display import HTML\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pdb\n",
    "\n",
    "import keras\n",
    "from keras import models\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "\n",
    "import itertools\n",
    "from itertools import permutations\n",
    "\n",
    "import IPython\n",
    "# print(IPython.sys_info())\n",
    "\n",
    "\n",
    "import math\n",
    "import time\n",
    "import seaborn as sns\n",
    "\n",
    "import fancyimpute\n",
    "from fancyimpute import KNN\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "print(\"Pandas \", pd.__version__)\n",
    "print(\"Numpy \", np.__version__)\n",
    "\n",
    "#Importing Pandas\n",
    "Raw_Total_Data = pd.read_csv(r\"C:\\Users\\ryan.hagan\\Desktop\\01_Preprocessed_WaterQuality_Data_2015_2019.csv\", encoding = \"ISO-8859-1\")\n",
    "                             \n",
    "Raw_Target_Data = pd.read_csv(r\"C:\\Users\\ryan.hagan\\Desktop\\01_Preprocessed_WaterQuality_Data_2015_2019.csv\", encoding = \"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of features:  82\n",
      "Total number of samples:  235\n"
     ]
    }
   ],
   "source": [
    "#Forcing data to be float\n",
    "Raw_Total_Data.iloc[:, 1:] = Raw_Total_Data.iloc[:, 1:].apply(pd.to_numeric, downcast = 'float', errors = 'coerce')\n",
    "Cleaned_Total_Data = Raw_Total_Data.iloc[:,1:].dropna()\n",
    "\n",
    "Raw_Target_Data.iloc[:, 1:] = Raw_Target_Data.iloc[:, 1:].apply(pd.to_numeric, downcast = 'float', errors = 'coerce')\n",
    "Cleaned_Target_Data = Raw_Target_Data.iloc[:,1:].dropna()\n",
    "\n",
    "n_features = Cleaned_Target_Data.shape[1]\n",
    "n_samples = Cleaned_Target_Data.shape[0]\n",
    "\n",
    "\n",
    "\n",
    "print('Total number of features: ', str(n_features))\n",
    "print('Total number of samples: ', str(n_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First Algorithm to use is DBSCAN\\n\",\n",
    "# #This is to check if the data geometry is uneven and clusters cannot be convex\\n\",\n",
    "# #Applying to the whole system \\n\",\n",
    "# DBSCAN_Output_System = sklearn.cluster.DBSCAN(metric='euclidean', algorithm='auto', n_jobs=-1).fit_predict(Cleaned_Total_Data)\n",
    "\n",
    "# #Applying to one data stream at a time\\n\",\n",
    "# DBSCAN_Output_Individual_Stream = Cleaned_Total_Data.copy()\n",
    "# for Counter in range(0, Cleaned_Total_Data.shape[1]):\n",
    "#     Input_Array = Cleaned_Total_Data.iloc[:,Counter].values\n",
    "#     DBSCAN_Output_Individual_Stream.iloc[:, Counter] = sklearn.cluster.DBSCAN(metric='euclidean', algorithm='auto', n_jobs=-1).fit_predict(Input_Array.reshape(-1,1))\n",
    "\n",
    "    \n",
    "# DBSCAN_Silhoutte = metrics.silhouette_score(Cleaned_Total_Data, DBSCAN_Output_System)\n",
    "# print(DBSCAN_Silhoutte)\n",
    "# print('Results dimensions: ', str(DBSCAN_Output_System.ndim))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Second algorithm to apply is Birch \n",
    "#This is a robust clustering algorithm used to detect outliers\n",
    "Birch_Output_System = sklearn.cluster.Birch(n_clusters=2, compute_labels=True, copy=True).fit_predict(Cleaned_Total_Data)\n",
    "Birch_Output_System3 = sklearn.cluster.Birch(n_clusters=3, compute_labels=True, copy=True).fit_predict(Cleaned_Total_Data)\n",
    "\n",
    "#Applying to one data stream at a time\n",
    "Birch_Output_Individual_Stream = Cleaned_Total_Data.copy()\n",
    "Birch_Output_Individual_Stream3 = Cleaned_Total_Data.copy()\n",
    "\n",
    "for Counter in range(0, Cleaned_Total_Data.shape[1]):\n",
    "    Input_Array = Cleaned_Total_Data.iloc[:,Counter].values\n",
    "    Birch_Output_Individual_Stream.iloc[:, Counter] = sklearn.cluster.Birch(n_clusters=2, compute_labels=True, copy=True).fit_predict(Input_Array.reshape(-1,1))\n",
    "    Birch_Output_Individual_Stream3.iloc[:, Counter] = sklearn.cluster.Birch(n_clusters=3, compute_labels=True, copy=True).fit_predict(Input_Array.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Third algorithm is agglomorative clustering\n",
    "#Applying to the whole system\n",
    "AggCluster_Output_System = sklearn.cluster.AgglomerativeClustering(n_clusters=2, affinity='euclidean', compute_full_tree='auto', linkage='ward').fit_predict(Cleaned_Total_Data)\n",
    "\n",
    "#Applying to one data stream at a time\n",
    "AggCluster_Output_Individual_Stream = Cleaned_Total_Data.copy()\n",
    "for Counter in range(0, Cleaned_Total_Data.shape[1]):\n",
    "    Input_Array = Cleaned_Total_Data.iloc[:,Counter].values\n",
    "    AggCluster_Output_Individual_Stream.iloc[:, Counter] = sklearn.cluster.AgglomerativeClustering(n_clusters=2, affinity='euclidean', compute_full_tree='auto', linkage='ward').fit_predict(Input_Array.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying the ensembling algoritm\n",
    "\n",
    "#Analyzing overall system state\n",
    "#First agglomerating features\n",
    "Ensemble_System_Input = np.zeros((int(AggCluster_Output_System.shape[0]),3))\n",
    "Ensemble_System_Input[:,0] = AggCluster_Output_System\n",
    "Ensemble_System_Input[:,1] = Birch_Output_System\n",
    "# Ensemble_System_Input[:,2] = DBSCAN_Output_System\n",
    "Ensemble_System_TempStorage = sklearn.cluster.FeatureAgglomeration(n_clusters=2, affinity='euclidean', \n",
    "                                                              compute_full_tree='auto', linkage='ward').fit_transform(Ensemble_System_Input)\n",
    "#Then applying clustering\n",
    "Ensemble_Output_System = sklearn.cluster.AgglomerativeClustering(n_clusters=2, affinity='euclidean', compute_full_tree='auto', linkage='ward').fit_predict(Ensemble_System_TempStorage)\n",
    "\n",
    "#Analyzing Individual Data Streams\n",
    "Ensemble_Output_Individual_Stream = Cleaned_Total_Data.copy()\n",
    "for Counter in range(0, Cleaned_Total_Data.shape[1]):\n",
    "    Ensemble_Input_Individual_Stream = np.zeros((int(AggCluster_Output_System.shape[0]),3))\n",
    "    Ensemble_Input_Individual_Stream[:,0] = AggCluster_Output_Individual_Stream.iloc[:,Counter].values\n",
    "    Ensemble_Input_Individual_Stream[:,1] = Birch_Output_Individual_Stream.iloc[:,Counter].values\n",
    "#     Ensemble_Input_Individual_Stream[:,2] = DBSCAN_Output_Individual_Stream.iloc[:,Counter].values\n",
    "    \n",
    "    Ensemble_Individual_Stream_TempStorage = sklearn.cluster.FeatureAgglomeration(n_clusters=2, affinity='euclidean', \n",
    "                                                                       compute_full_tree='auto', linkage='ward').fit_transform(Ensemble_Input_Individual_Stream)\n",
    "    Ensemble_Output_Individual_Stream.iloc[:, Counter] = sklearn.cluster.AgglomerativeClustering(n_clusters=2, affinity='euclidean', compute_full_tree='auto', linkage='ward').fit_predict(Ensemble_Individual_Stream_TempStorage)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble Model Silhoutte Score is 0.37830228 AggCluster Silhoutte Score is 0.37830228 Birch Silhoutte Score is 0.37830228\n"
     ]
    }
   ],
   "source": [
    "EnsembleModel_Silhoutte = metrics.silhouette_score(Cleaned_Total_Data, Ensemble_Output_System, metric='euclidean')\n",
    "Aggcluster_Silhoutte = metrics.silhouette_score(Cleaned_Total_Data, AggCluster_Output_System, metric='euclidean')\n",
    "Birch_Silhoutte = metrics.silhouette_score(Cleaned_Total_Data, Birch_Output_System, metric='euclidean')\n",
    "\n",
    "#DBSCAN_Silhoutte = metrics.silhouette_score(Cleaned_Total_Data, DBSCAN_Output_System)\n",
    "\n",
    "print('Ensemble Model Silhoutte Score is ' + str(EnsembleModel_Silhoutte) + ' AggCluster Silhoutte Score is ' + str(Aggcluster_Silhoutte) + ' Birch Silhoutte Score is ' + str(Birch_Silhoutte))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "EnsembleModel_Silhoutte = metrics.davies_bouldin_score(Cleaned_Total_Data, Ensemble_Output_System)\n",
    "Aggcluster_Silhoutte = metrics.davies_bouldin_score(Cleaned_Total_Data, AggCluster_Output_System)\n",
    "Birch_Silhoutte = metrics.davies_bouldin_score(Cleaned_Total_Data, Birch_Output_System)\n",
    "#DBSCAN_Silhoutte = metrics.davies_bouldin_score(Cleaned_Total_Data, DBSCAN_Output_System)\n",
    "\n",
    "# print('Ensemble Model Davies-Bouldin Index is ' + str(EnsembleModel_Silhoutte) + ' AggCluster Davies-Bouldin Index is ' + str(Aggcluster_Silhoutte) + ' Birch Davies-Bouldin Index is ' + str(Birch_Silhoutte))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the results of Anomaly Detection Algorithm\n",
    "\n",
    "Ensemble_Output_System_Dataframe = pd.DataFrame(data= Ensemble_Output_System, index = Cleaned_Total_Data.index.values)\n",
    "Birch_Output_System_Dataframe = pd.DataFrame(data= Birch_Output_System, index = Cleaned_Total_Data.index.values)\n",
    "Ensemble_Output_Individual_Stream_Dataframe = pd.DataFrame(data= Ensemble_Output_Individual_Stream, index = Cleaned_Total_Data.index.values,\n",
    "                                    columns = Cleaned_Total_Data.columns.values)\n",
    "\n",
    "\n",
    "Birch_Output_Individual_Stream_Dataframe = pd.DataFrame(data= Birch_Output_Individual_Stream, index = Cleaned_Total_Data.index.values,\n",
    "                                    columns = Cleaned_Total_Data.columns.values)\n",
    "\n",
    "\n",
    "Agg_Output_System_Dataframe = pd.DataFrame(data= AggCluster_Output_System, index = Cleaned_Total_Data.index.values)\n",
    "\n",
    "Agg_Output_Individual_Stream_Dataframe = pd.DataFrame(data= AggCluster_Output_Individual_Stream, index = Cleaned_Total_Data.index.values,\n",
    "                                    columns = Cleaned_Total_Data.columns.values)\n",
    "\n",
    "\n",
    "\n",
    "Ensemble_Output_System_Dataframe.to_csv(r\"C:\\Users\\ryan.hagan\\Desktop\\01_WaterQuality_System_Status_2019.csv\")\n",
    "Birch_Output_System_Dataframe.to_csv(r\"C:\\Users\\ryan.hagan\\Desktop\\01_WaterQuality_System_Status_All_BIRCH.csv\")\n",
    "Birch_Output_Individual_Stream_Dataframe.to_csv(r\"C:\\Users\\ryan.hagan\\Desktop\\01_WaterQuality_Stream_Status_ALL_BIRCH_INDIVIDUAL.csv\")\n",
    "Ensemble_Output_Individual_Stream_Dataframe.to_csv(r\"C:\\Users\\ryan.hagan\\Desktop\\01_WaterQuality_Stream_Status_2019.csv\")\n",
    "\n",
    "Agg_Output_System_Dataframe.to_csv(r\"C:\\Users\\ryan.hagan\\Desktop\\01_WaterQuality_System_Status_All_Aggcluster.csv\")\n",
    "Agg_Output_Individual_Stream_Dataframe.to_csv(r\"C:\\Users\\ryan.hagan\\Desktop\\01_WaterQuality_Stream_Status_ALL_Aggcluster_INDIVIDUAL.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
